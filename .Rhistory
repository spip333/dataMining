# prepare data
library("e1071")
library(class)
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:10000,]
dim(mnist_matrix)
sort(unique(mnist_matrix[,1]))
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:100){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
mnist_matrix =
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
mnist_matrix =
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:10000,]
dim(mnist_matrix)
sort(unique(mnist_matrix[,1]))
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:100){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
for(i in 1:100){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
# KNN
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
table(pred_knn, test_labels)
head(pred_knn, 20)
head(test_labels, 20)
truth_vector_validate_knn = pred_knn == test_labels
good_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==TRUE])
bad_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==FALSE])
bad_knn / (good_knn+bad_knn)
m = naiveBayes(trainData, train_labels)
pred_nb = predict(m, testData)
m = naiveBayes(train_data, train_labels)
pred_nb = predict(m, test_data)
table(test_labels, pred_nb)
truth_vector_validate_nb = pred_nb == test_labels
good_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==TRUE])
bad_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==FALSE])
bad_nb/(good_nb+bad_nb)
# prepare data
library("e1071")
library(class)
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:1000,]
dim(mnist_matrix)
sort(unique(mnist_matrix[,1]))
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:100){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
# KNN
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
table(pred_knn, test_labels)
head(pred_knn, 20)
head(test_labels, 20)
truth_vector_validate_knn = pred_knn == test_labels
good_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==TRUE])
bad_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==FALSE])
bad_knn / (good_knn+bad_knn)
# naive-bayes
m = naiveBayes(train_data, train_labels)
pred_nb = predict(m, test_data)
table(test_labels, pred_nb)
truth_vector_validate_nb = pred_nb == test_labels
good_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==TRUE])
bad_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==FALSE])
bad_nb/(good_nb+bad_nb)
# prepare data
library("e1071")
library(class)
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:1000,]
dim(mnist_matrix)
sort(unique(mnist_matrix[,1]))
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:100){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
# KNN
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
table(pred_knn, test_labels)
head(pred_knn, 20)
head(test_labels, 20)
truth_vector_validate_knn = pred_knn == test_labels
good_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==TRUE])
bad_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==FALSE])
bad_knn / (good_knn+bad_knn)
# naive-bayes
m = naiveBayes(train_data, train_labels)
pred_nb = predict(m, test_data)
table(test_labels, pred_nb)
truth_vector_validate_nb = pred_nb == test_labels
good_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==TRUE])
bad_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==FALSE])
bad_nb/(good_nb+bad_nb)
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:10){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
28*28
?naiveBayes
round(1.777777, 2)
# prepare data
library("e1071")
library(class)
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:600,]
dim(mnist_matrix)
sort(unique(mnist_matrix[,1]))
par( mfrow = c(10, 10), mai = c(0,0,0,0))
for(i in 1:10){
y = as.matrix(mnist_matrix[i, 2:785])
dim(y) = c(28, 28)
image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))
text( 0.2, 0, mnist_matrix[i,1], cex = 3, col = 2, pos = c(3,4))
}
28*28
# prepare data
ind = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[ind==1,]
test_data_with_label = mnist_matrix[ind==2,]
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
# KNN
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
table(pred_knn, test_labels)
head(pred_knn, 20)
head(test_labels, 20)
truth_vector_validate_knn = pred_knn == test_labels
good_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==TRUE])
bad_knn = length(truth_vector_validate_knn[truth_vector_validate_knn==FALSE])
bad_knn / (good_knn+bad_knn)
# naive-bayes
m = naiveBayes(train_data, train_labels)
pred_nb = predict(m, test_data)
table(test_labels, pred_nb)
truth_vector_validate_nb = pred_nb == test_labels
good_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==TRUE])
bad_nb = length(truth_vector_validate_nb[truth_vector_validate_nb==FALSE])
result_nb <- good_nb/(good_nb+bad_nb)
round(result_nb, 2)
library("e1071")
library(class)
mnist_matrix_full = read.csv( 'MNIST.csv' )
mnist_matrix = mnist_matrix_full[1:500,]
# Erstellen eines Samples fÃ¼r die Auftrennung Train- / Test -Daten
index = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.1, 0.9))
train_data_with_label = mnist_matrix[index==1,]
test_data_with_label = mnist_matrix[index==2,]
# Spalte mit Labels weglassen
train_data = train_data_with_label[-1]
test_data = test_data_with_label[-1]
# Vector der Labels erstellen
train_labels = factor(train_data_with_label[,1])
test_labels = factor(test_data_with_label[,1])
pred_knn = knn(train = train_data,
test = test_data,
cl= train_labels,
k = 10,
prob=TRUE)
table(pred_knn, test_labels)
# Auswertung
validate_knn = pred_knn == test_labels
good_knn = length(validate_knn[validate_knn==TRUE])
bad_knn = length(validate_knn[validate_knn==FALSE])
prop_good_knn <- good_knn / (good_knn + bad_knn)
# Versuch einer Auswertung mit Naive-Bayes
model = naiveBayes(train_data, train_labels)
pred_nb = predict(model, test_data)
table(test_labels, pred_nb)
validate_nb = pred_nb == test_labels
good_nb = length(validate_nb[validate_nb == TRUE])
bad_nb  = length(validate_nb[validate_nb == FALSE])
prop_good_nb <- good_nb / (good_nb + bad_nb)
prop_good_nb
index2 = sample(2, nrow(mnist_matrix), replace=TRUE, prob=c(0.3, 0.7))
train_data_with_label2 = mnist_matrix[index2==1,]
test_data_with_label2 = mnist_matrix[index2==2,]
# Spalte mit Labels weglassen
train_data2 = train_data_with_label2[-1]
test_data2 = test_data_with_label2[-1]
# Vector der Labels erstellen
train_labels2 = factor(train_data_with_label2[,1])
test_labels2 = factor(test_data_with_label2[,1])
pred_knn2 = knn(train = train_data2,
test = test_data2,
cl= train_labels2,
k = 10,
prob=TRUE)
table(pred_knn2, test_labels2)
# Auswertung
validate_knn2 = pred_knn2 == test_labels2
good_knn2 = length(validate_knn2[validate_knn2==TRUE])
bad_knn2 = length(validate_knn2[validate_knn2==FALSE])
prop_good_knn2<- good_knn2 / (good_knn2 + bad_knn2)
model2 = naiveBayes(train_data2, train_labels2)
pred_nb2 = predict(model2, test_data2)
table(test_labels2, pred_nb2)
validate_nb2 = pred_nb2 == test_labels2
good_nb2 = length(validate_nb2[validate_nb2 == TRUE])
bad_nb2  = length(validate_nb2[validate_nb2 == FALSE])
prop_good_nb2 <- good_nb2 / (good_nb2 + bad_nb2)
table(pred_knn2, test_labels2)
